## Installation

Please follow the instructions below to install the repo and dependencies.

```bash
git clone https://github.com/HengyiWang/Co-SLAM.git
cd Co-SLAM
```

### Install the environment

```bash
# Create conda environment
conda create -n coslam python=3.7
conda activate coslam

# Install the pytorch first (Please check the cuda version)
pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html

# Install all the dependencies via pip (Note here pytorch3d and tinycudann requires ~10min to build)
pip install -r requirements.txt

# Build extension (marching cubes from neuralRGBD)
cd external/NumpyMarchingCubes
python setup.py install
```

For tinycudann, if you cannot access network when you use GPUs, you can also try build from source as below:

```bash
# Build tinycudann
git clone --recursive https://github.com/nvlabs/tiny-cuda-nn

# Try this version if you cannot use the latest version of tinycudann
#git reset --hard 91ee479d275d322a65726435040fc20b56b9c991
cd tiny-cuda-nn/bindings/torch
python setup.py install
```

## Dataset

#### Replica

Download the sequences of the Replica Dataset generated by the authors of iMAP into `./data/Replica` folder.

```bash
bash scripts/download_replica.sh # Released by authors of NICE-SLAM
```

#### ScanNet

Please follow the procedure on [ScanNet](http://www.scan-net.org/) website, and extract color & depth frames from the `.sens` file using the [code](https://github.com/ScanNet/ScanNet/blob/master/SensReader/python/reader.py).

#### Synthetic RGB-D dataset

Download the sequences of the synethetic RGB-D dataset generated by the authors of neuralRGBD into `./data/neural_rgbd_data` folder. The scenes with NaN poses generated by BundleFusion are excluded.

```bash
bash scripts/download_rgbd.sh
```

#### TUM RGB-D

Download 3 sequences of TUM RGB-D dataset into `./data/TUM` folder.

```bash
bash scripts/download_tum.sh
```

### Running Co-SLAM

1. Prepare your dataset in the required format.
2. Configure parameters in `config.yaml`
3. Run Co-SLAM:
   ```
   python coslam.py --config './configs/{Dataset}/{scene}.yaml'
   ```

### Running the Visualizer

1. Ensure you have completed a Co-SLAM run for the dataset you want to visualize.
2. Make sure the configuration file for the dataset is properly set up.
3. Run the visualizer:
   ```
   python visualizer.py --config './configs/{Dataset}/{scene}.yaml'
   ```

## Evaluation

This implementation employs a slightly different evaluation strategy to measure the quality of the reconstruction. You can find the code [here](https://github.com/JingwenWang95/neural_slam_eval). Note if you want to follow the evaluation protocol of NICE-SLAM, please refer to the supplementary material of the Co-SLAM paper for detailed parameters setting.